import streamlit as st
import os
import time
import requests
from dotenv import load_dotenv
from openai import OpenAI
from pytrends.request import TrendReq
import json
import pandas as pd
from io import BytesIO
from docx import Document
from docx.shared import Pt, RGBColor, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
import re
import html

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="Global SEO Marketing Pro", 
    page_icon="ğŸš¢", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== ì‚¬ì´ë“œë°” ë„¤ë¹„ê²Œì´ì…˜ ====================
import base64

# âœ… ì‚¬ì´ë“œë°” CSS ì¶”ê°€ (ê¸°ì¡´ CSSì™€ í†µí•©)
st.markdown("""
    <style>
    /* ì‚¬ì´ë“œë°” ë¡œê³  ìŠ¤íƒ€ì¼ */
    .logo-box{
        background: rgba(255,255,255,0.6);
        border: 1px solid #e5e7eb;
        border-radius: 14px;
        padding: 14px 12px;
        margin-bottom: 10px;
        text-align:center;
    }
    .logo-img{
        max-width: 150px;
        width: 100%;
        height: auto;
        display:block;
        margin: 0 auto;
    }
    .small-muted{
        color:#64748b;
        font-size: 0.85rem;
        font-weight: 600;
        letter-spacing: 0.2px;
    }
    /* Streamlit ê¸°ë³¸ ë„¤ë¹„ê²Œì´ì…˜ ìˆ¨ê¹€ */
    [data-testid="stSidebarNav"] {
        display: none;
    }
    section[data-testid="stSidebar"] { 
        background: #ffffff !important;
        border-right: 1px solid #e5e7eb;
    }
    </style>
""", unsafe_allow_html=True)

st.markdown("""
    <style>
    /* Streamlit ê¸°ë³¸ ë„¤ë¹„ê²Œì´ì…˜ ìˆ¨ê¹€ */
    [data-testid="stSidebarNav"] {
        display: none;
    }
    
    /* ì‚¬ì´ë“œë°” ë°°ê²½ */
    section[data-testid="stSidebar"] { 
        background: #ffffff !important;
        border-right: 1px solid #e5e7eb;
    }
    
    /* ì‚¬ì´ë“œë°” ë²„íŠ¼ ìŠ¤íƒ€ì¼ (âœ… hover íš¨ê³¼ ì¶”ê°€) */
    .stButton>button {
        background: #051161 !important;
        color: #ffffff !important;
        border: none !important;
        border-radius: 8px;
        padding: 10px 14px;
        font-weight: 700;
        transition: 0.3s;
    }
    .stButton>button:hover {
        background: rgba(5,17,97,0.85) !important;
        box-shadow: 0 4px 12px rgba(5,17,97,0.3) !important;
    }
    
    /* ì‚¬ì´ë“œë°” ë¡œê³  ìŠ¤íƒ€ì¼ */
    .logo-box{
        background: rgba(255,255,255,0.6);
        border: 1px solid #e5e7eb;
        border-radius: 14px;
        padding: 14px 12px;
        margin-bottom: 10px;
        text-align:center;
    }
    .logo-img{
        max-width: 150px;
        width: 100%;
        height: auto;
        display:block;
        margin: 0 auto;
    }
    .small-muted{
        color:#64748b;
        font-size: 0.85rem;
        font-weight: 600;
        letter-spacing: 0.2px;
    }
    </style>
""", unsafe_allow_html=True)

with st.sidebar:
    # 1) êµ­ê°€ë³„ ìˆ˜ì¶œì… ë°ì´í„°
    with st.expander("1) í•´ì™¸ì§„ì¶œ ì „ëµ í—ˆë¸Œ", expanded=False):
        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("ì‹œì¥ë™í–¥", use_container_width=True, key="nav_cn_1"):
                st.switch_page("pages/macro_1.py")
        with col2:
            if st.button("ì „ëµë¶„ì„", use_container_width=True, key="nav_cn_2"):
                st.switch_page("pages/micro_1.py")
        with col3:
            if st.button("ê·œì œì§„ë‹¨", use_container_width=True, key="nav_cn_3"):
                st.switch_page("pages/mac_mic_1.py")

    # 2) SEO ì„œë¹„ìŠ¤
    with st.expander("2) SEO ì„œë¹„ìŠ¤", expanded=False):
        if st.button("ë°”ë¡œê°€ê¸°", use_container_width=True, key="nav_news"):
            st.switch_page("pages/junghyun.py")

    # 3) AI ë°”ì´ì–´ ë§¤ì¹­ ì„œë¹„ìŠ¤
    with st.expander("3) AI ë°”ì´ì–´ ë§¤ì¹­ ì„œë¹„ìŠ¤", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ë°”ì´ì–´ ì°¾ê¸°", use_container_width=True, key="nav_ai_1"):
                st.switch_page("pages/03_ai_chatbot.py")
        with col2:
            if st.button("ì „ì‹œíšŒ ì¼ì •", use_container_width=True, key="nav_ai_2"):
                st.switch_page("pages/buyer_maps.py")

    # 4) í™˜ìœ¨ ì •ë³´ í™•ì¸
    with st.expander("4) í™˜ìœ¨ ì •ë³´ í™•ì¸", expanded=False):
        if st.button("ë°”ë¡œê°€ê¸°", use_container_width=True, key="nav_ex"):
            st.switch_page("pages/exchange_rate.py")

    # 5) ë¬´ì—­ ì„œë¥˜ ìë™ ì™„ì„±
    with st.expander("5) ë¬´ì—­ ì„œë¥˜ ìë™ ì™„ì„±", expanded=False):
        if st.button("ë°”ë¡œê°€ê¸°", use_container_width=True, key="nav_fx"):
            st.switch_page("pages/auto_docs.py")

    # ë¡œê³  ì˜ì—­
    logo_path = "assets/logo.png"
    if os.path.exists(logo_path):
        logo_b64 = base64.b64encode(open(logo_path, "rb").read()).decode()
        st.markdown(
            f"""
            <div class="logo-box">
              <img class="logo-img" src="data:image/png;base64,{logo_b64}" alt="logo"/>
              <div class="small-muted" style="margin-top:8px; text-align:center;">
                KITA AX MASTER TEAM4
              </div>
            </div>
            """,
            unsafe_allow_html=True,
        )

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("---")

    # âœ… í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼
    if st.button("ğŸ  í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸°", use_container_width=True, key="go_home_sidebar"):
        st.switch_page("dashboard.py")

# --- CSS ìŠ¤íƒ€ì¼ (í–¥ìƒëœ ë””ìì¸) ---
st.markdown("""
    <style>
    .main { background-color: #f5f7fa; }
    .stButton>button { 
        width: 100%; 
        background: #051161;
        color: white; 
        font-weight: bold; 
        border: none; 
        padding: 14px; 
        border-radius: 10px;
        font-size: 16px;
        transition: all 0.3s;
    }
    .stButton>button:hover { 
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
    }
    .keyword-card { 
        background: white; 
        padding: 24px; 
        border-radius: 16px; 
        border: 1px solid #e1e8ed; 
        box-shadow: 0 4px 6px rgba(0,0,0,0.07);
        margin-bottom: 20px;
    }
    .metric-box {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        color: white;
        padding: 15px;
        border-radius: 12px;
        text-align: center;
        margin: 10px 0;
    }
    .keyword-tag {
        display: inline-block;
        background-color: #e3f2fd;
        color: #1976d2;
        padding: 8px 16px;
        border-radius: 20px;
        margin: 5px;
        font-weight: 500;
    }

    .keyword-eng {
        display: block;
        color: #8a8f98;
        font-size: 12px;
        margin-top: 4px;
        font-weight: 400;
    }
    h1, h2, h3, h4 { color: #1a202c; font-weight: 700; }
    .info-badge {
        background-color: #fff3cd;
        color: #856404;
        padding: 4px 12px;
        border-radius: 12px;
        font-size: 0.85em;
        font-weight: 600;
    }
    </style>
    """, unsafe_allow_html=True)

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPEN_API_KEY") or os.getenv("OPENAI_API_KEY")
SERPAPI_KEY = os.getenv("SERPAPI_KEY")
DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")

# âœ… ì˜ì–´ê¶Œ êµ­ê°€ ì •ì˜(í•œ ë²ˆë§Œ)
ENGLISH_COUNTRIES = ['US', 'GB', 'AU', 'CA', 'NZ', 'SG', 'IE', 'ZA', 'NG', 'PH', 'IN']


# --- ë©”ì¸ í—¤ë” ---
st.title("ğŸš¢ SEO ë§ˆì¼€íŒ… & ì½˜í…ì¸  ìƒì„±")
st.markdown("""
<div style='background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 12px; color: white; margin-bottom: 30px;'>
    <h4 style='color: white; margin: 0;'> ë¶„ì„ ë°©ì‹</h4>
    <ul style='margin: 10px 0; padding-left: 20px;'>
        <li>ê²½ìŸì‚¬ ìœ ë£Œ ê´‘ê³  í‚¤ì›Œë“œ ì—­ë¶„ì„</li>
        <li>ë¸Œëœë“œëª…Â·ìš©ëŸ‰ ìë™ ì œì™¸í•œ ìˆœìˆ˜ ë§ˆì¼€íŒ… í‚¤ì›Œë“œ ì¶”ì¶œ</li>
        <li>STP ë¶„ì„ ë° í•µì‹¬ íƒ€ê²Ÿ ì†Œë¹„ì¸µ ì„¤ì •</li>
        <li>ì•„ë§ˆì¡´Â·ìì‚¬ëª°Â·SNS ì½˜í…ì¸  í˜„ì§€í™” ìƒì„±</li>
    </ul>
</div>
""", unsafe_allow_html=True)

# --- ì…ë ¥ ì„¹ì…˜ ---
st.subheader("ì œí’ˆ ì •ë³´ ì…ë ¥")

col1, col2, col3 = st.columns([1.2, 1.5, 1])

with col1:
    input_type = st.selectbox(
        "ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["ì œí’ˆ í‚¤ì›Œë“œ", "HS Code"],
        help="HS CodeëŠ” 6ìë¦¬ êµ­ì œ ê´€ì„¸ ì½”ë“œì…ë‹ˆë‹¤"
    )

with col2:
    if input_type == "HS Code":
        placeholder_text = "ì˜ˆ: 190230 (íŒŒìŠ¤íƒ€), 851830 (í—¤ë“œí°)"
    else:
        placeholder_text = "ì˜ˆ: ë¬´ì„  ì´ì–´í°, í”„ë¡œí‹´ íŒŒìš°ë”"
    
    user_input = st.text_input(
        "ì œí’ˆëª… ë˜ëŠ” HS Code",
        placeholder=placeholder_text
    )

with col3:
    # êµ­ê°€ë¥¼ ì§€ì—­ë³„ë¡œ ê·¸ë£¹í™”
    country_options = {
        "ë¶ë¯¸": ["US", "CA", "MX"],
        "ìœ ëŸ½": ["GB", "DE", "FR", "IT", "ES", "NL", "BE", "AT", "CH", "SE", "NO", "DK", "FI", "PL", "IE", "PT", "GR", "CZ", "RO", "HU"],
        "ì•„ì‹œì•„": ["JP", "KR", "CN", "SG", "TW", "HK", "TH", "MY", "ID", "VN", "PH", "IN"],
        "ì˜¤ì„¸ì•„ë‹ˆì•„": ["AU", "NZ"],
        "ì¤‘ë™": ["AE", "SA", "IL", "TR"],
        "ë‚¨ë¯¸": ["BR", "AR", "CL", "CO"],
        "ì•„í”„ë¦¬ì¹´": ["ZA", "EG", "NG"]
    }
    
    # ì „ì²´ êµ­ê°€ ë¦¬ìŠ¤íŠ¸ (ì•ŒíŒŒë²³ ìˆœ)
    all_countries = []
    for countries in country_options.values():
        all_countries.extend(countries)
    all_countries = sorted(list(set(all_countries)))
    
    target_country = st.selectbox(
        "íƒ€ê²Ÿ êµ­ê°€",
        all_countries,
        help="ë§ˆì¼€íŒ… ëŒ€ìƒ êµ­ê°€ë¥¼ ì„ íƒí•˜ì„¸ìš” (60ê°œêµ­ ì´ìƒ ì§€ì›)"
    )

analyze_btn = st.button("ì „ì²´ ì‹œì¥ ë¶„ì„ ì‹œì‘", use_container_width=True)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def get_language_code(country_code):
    """êµ­ê°€ë³„ ì–¸ì–´ ì½”ë“œ ë§¤í•‘ (í™•ì¥)"""
    lang_map = {
        # ë¶ë¯¸
        'US': 'en', 'CA': 'en', 'MX': 'es',
        # ìœ ëŸ½
        'GB': 'en', 'IE': 'en',
        'DE': 'de', 'AT': 'de', 'CH': 'de',
        'FR': 'fr', 'BE': 'fr',
        'IT': 'it', 'ES': 'es', 'PT': 'pt',
        'NL': 'nl', 'SE': 'sv', 'NO': 'no', 'DK': 'da', 'FI': 'fi',
        'PL': 'pl', 'CZ': 'cs', 'RO': 'ro', 'HU': 'hu', 'GR': 'el',
        # ì•„ì‹œì•„
        'JP': 'ja', 'KR': 'ko', 'CN': 'zh-CN', 'TW': 'zh-TW', 'HK': 'zh-HK',
        'SG': 'en', 'TH': 'th', 'MY': 'ms', 'ID': 'id', 'VN': 'vi', 'PH': 'en',
        'IN': 'en',
        # ì˜¤ì„¸ì•„ë‹ˆì•„
        'AU': 'en', 'NZ': 'en',
        # ì¤‘ë™
        'AE': 'ar', 'SA': 'ar', 'IL': 'he', 'TR': 'tr',
        # ë‚¨ë¯¸
        'BR': 'pt', 'AR': 'es', 'CL': 'es', 'CO': 'es',
        # ì•„í”„ë¦¬ì¹´
        'ZA': 'en', 'EG': 'ar', 'NG': 'en'
    }
    return lang_map.get(country_code, 'en')

def get_language_name(country_code):
    """êµ­ê°€ë³„ ì–¸ì–´ ì´ë¦„ ë§¤í•‘ (í™•ì¥)"""
    lang_map = {
        # ë¶ë¯¸
        'US': 'English (US)', 'CA': 'English (CA)', 'MX': 'Spanish (Mexico)',
        # ìœ ëŸ½
        'GB': 'English (UK)', 'IE': 'English (Ireland)',
        'DE': 'German', 'AT': 'German (Austria)', 'CH': 'German (Switzerland)',
        'FR': 'French', 'BE': 'French (Belgium)',
        'IT': 'Italian', 'ES': 'Spanish', 'PT': 'Portuguese',
        'NL': 'Dutch', 'SE': 'Swedish', 'NO': 'Norwegian', 'DK': 'Danish', 'FI': 'Finnish',
        'PL': 'Polish', 'CZ': 'Czech', 'RO': 'Romanian', 'HU': 'Hungarian', 'GR': 'Greek',
        # ì•„ì‹œì•„
        'JP': 'Japanese', 'KR': 'Korean', 'CN': 'Chinese (Simplified)', 
        'TW': 'Chinese (Traditional)', 'HK': 'Chinese (Hong Kong)',
        'SG': 'English (Singapore)', 'TH': 'Thai', 'MY': 'Malay', 
        'ID': 'Indonesian', 'VN': 'Vietnamese', 'PH': 'English (Philippines)',
        'IN': 'English (India)',
        # ì˜¤ì„¸ì•„ë‹ˆì•„
        'AU': 'English (Australia)', 'NZ': 'English (New Zealand)',
        # ì¤‘ë™
        'AE': 'Arabic (UAE)', 'SA': 'Arabic (Saudi Arabia)', 'IL': 'Hebrew', 'TR': 'Turkish',
        # ë‚¨ë¯¸
        'BR': 'Portuguese (Brazil)', 'AR': 'Spanish (Argentina)', 
        'CL': 'Spanish (Chile)', 'CO': 'Spanish (Colombia)',
        # ì•„í”„ë¦¬ì¹´
        'ZA': 'English (South Africa)', 'EG': 'Arabic (Egypt)', 'NG': 'English (Nigeria)'
    }
    return lang_map.get(country_code, 'English')

def get_country_name(country_code):
    """êµ­ê°€ ì½”ë“œ â†’ êµ­ê°€ëª… (í™•ì¥)"""
    country_map = {
        # ë¶ë¯¸
        'US': 'United States', 'CA': 'Canada', 'MX': 'Mexico',
        # ìœ ëŸ½
        'GB': 'United Kingdom', 'IE': 'Ireland',
        'DE': 'Germany', 'AT': 'Austria', 'CH': 'Switzerland',
        'FR': 'France', 'BE': 'Belgium',
        'IT': 'Italy', 'ES': 'Spain', 'PT': 'Portugal',
        'NL': 'Netherlands', 'SE': 'Sweden', 'NO': 'Norway', 'DK': 'Denmark', 'FI': 'Finland',
        'PL': 'Poland', 'CZ': 'Czech Republic', 'RO': 'Romania', 'HU': 'Hungary', 'GR': 'Greece',
        # ì•„ì‹œì•„
        'JP': 'Japan', 'KR': 'South Korea', 'CN': 'China', 'TW': 'Taiwan', 'HK': 'Hong Kong',
        'SG': 'Singapore', 'TH': 'Thailand', 'MY': 'Malaysia', 
        'ID': 'Indonesia', 'VN': 'Vietnam', 'PH': 'Philippines', 'IN': 'India',
        # ì˜¤ì„¸ì•„ë‹ˆì•„
        'AU': 'Australia', 'NZ': 'New Zealand',
        # ì¤‘ë™
        'AE': 'United Arab Emirates', 'SA': 'Saudi Arabia', 'IL': 'Israel', 'TR': 'Turkey',
        # ë‚¨ë¯¸
        'BR': 'Brazil', 'AR': 'Argentina', 'CL': 'Chile', 'CO': 'Colombia',
        # ì•„í”„ë¦¬ì¹´
        'ZA': 'South Africa', 'EG': 'Egypt', 'NG': 'Nigeria'
    }
    return country_map.get(country_code, country_code)

# --- í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

def get_seed_keyword(client, user_input, input_type, target_country_code):
    """
    HS Code ë˜ëŠ” ì œí’ˆëª… â†’ í˜„ì§€ ì–¸ì–´ í‘œì¤€ í’ˆëª©ëª… ë³€í™˜
    ê´€ì„¸ì‚¬(Customs Broker) í˜ë¥´ì†Œë‚˜ ì‚¬ìš©
    """
    lang_name = get_language_name(target_country_code)
    
    system_prompt = """You are an expert International Trade Specialist and Customs Broker with deep knowledge of HS Code classifications and global product nomenclature."""
    
    if input_type == "HS Code":
        user_prompt = f"""
Analyze HS CODE: '{user_input}'
Target Market: {target_country_code} ({lang_name})

TASK: Identify the **OFFICIAL GENERIC PRODUCT CATEGORY NAME** for this HS Code.

â›” STRICT PROHIBITIONS:
- NO specific brand names (e.g., 'Sony', 'Nike', 'Barilla')
- NO niche flavors or variants unless they define the main category
- NO product model numbers

âœ… REQUIRED OUTPUT:
- Most common, generic search term consumers use in {lang_name}
- Must be the broadest commercially viable category

Examples:
- Input '190230' â†’ Output: 'Pasta' (or 'ãƒ‘ã‚¹ã‚¿' for JP)
- Input '851830' â†’ Output: 'Headphones' (or 'ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³' for JP)
- Input '330210' â†’ Output: 'Perfume'

Output Format (2 words only, separated by comma):
NativeLanguageKeyword, EnglishKeyword
"""
    else:
        user_prompt = f"""
Product Name: '{user_input}'
Target Market: {target_country_code} ({lang_name})

TASK: Translate this product into the most common generic search term in {lang_name}.

â›” Remove any brand names or specific measurements.
âœ… Output the broadest product category term.

Output Format (2 words only, separated by comma):
NativeLanguageKeyword, EnglishKeyword
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3
        )
        content = response.choices[0].message.content.strip()
        content = content.replace('*', '').replace('"', '').replace("'", '').replace('`', '')
        
        if ',' in content:
            parts = [x.strip() for x in content.split(',', 1)]
            return parts[0], parts[1] if len(parts) > 1 else parts[0]
        
        return content.strip(), content.strip()
    except Exception as e:
        st.error(f"í‚¤ì›Œë“œ ì‹ë³„ ì‹¤íŒ¨: {str(e)}")
        return None, None


def fetch_comprehensive_serpapi_data(search_term, country_code):
    """
    SerpApi í†µí•© ë°ì´í„° ìˆ˜ì§‘:
    1. Google Shopping (ê²½ìŸì‚¬ ì œí’ˆëª…)
    2. Organic Search (ìƒìœ„ ë­í¬ ì‚¬ì´íŠ¸ì˜ ë©”íƒ€ ì„¤ëª…)
    3. Ads (ìœ ë£Œ ê´‘ê³  í‚¤ì›Œë“œ - ì „í™˜ í‚¤ì›Œë“œì˜ ë³´ê³ )
    4. People Also Ask (ê´€ë ¨ ì§ˆë¬¸ - ë¡±í…Œì¼ í‚¤ì›Œë“œ)
    5. Related Searches (êµ¬ê¸€ ì¶”ì²œ í‚¤ì›Œë“œ)
    """
    if not SERPAPI_KEY:
        return None
    
    lang_code = get_language_code(country_code)
    collected_data = {
        "shopping_titles": [],
        "ads_headlines": [],
        "organic_snippets": [],
        "people_also_ask": [],
        "related_searches": []
    }
    
    # 1. Google Shopping ê²€ìƒ‰
    try:
        params_shopping = {
            "engine": "google_shopping",
            "q": search_term,
            "gl": country_code,
            "hl": lang_code,
            "api_key": SERPAPI_KEY,
            "num": 30
        }
        
        res = requests.get("https://serpapi.com/search", params=params_shopping, timeout=15).json()
        
        if "shopping_results" in res:
            for item in res["shopping_results"][:30]:
                title = item.get("title", "")
                if title:
                    collected_data["shopping_titles"].append(title)
        
        time.sleep(0.5)  # Rate limiting
    except Exception as e:
        st.warning(f"Shopping ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    # 2. Google Organic + Ads ê²€ìƒ‰
    try:
        params_organic = {
            "engine": "google",
            "q": search_term,
            "gl": country_code,
            "hl": lang_code,
            "api_key": SERPAPI_KEY,
            "num": 20
        }
        
        res = requests.get("https://serpapi.com/search", params=params_organic, timeout=15).json()
        
        # 2-1. ìœ ë£Œ ê´‘ê³  (Ads) - ì „í™˜ í‚¤ì›Œë“œì˜ í•µì‹¬
        if "ads" in res:
            for ad in res["ads"][:10]:
                headline = ad.get("title", "") or ad.get("headline", "")
                if headline:
                    collected_data["ads_headlines"].append(headline)
        
        # 2-2. Organic ê²€ìƒ‰ ê²°ê³¼ (ë©”íƒ€ ì„¤ëª…)
        if "organic_results" in res:
            for item in res["organic_results"][:10]:
                snippet = item.get("snippet", "")
                if snippet:
                    collected_data["organic_snippets"].append(snippet)
        
        # 2-3. People Also Ask (ì§ˆë¬¸ í˜•íƒœ í‚¤ì›Œë“œ)
        if "related_questions" in res:
            for q in res["related_questions"][:10]:
                question = q.get("question", "")
                if question:
                    collected_data["people_also_ask"].append(question)
        
        # 2-4. Related Searches (êµ¬ê¸€ ì¶”ì²œ ê²€ìƒ‰ì–´)
        if "related_searches" in res:
            for rs in res["related_searches"][:10]:
                query = rs.get("query", "")
                if query:
                    collected_data["related_searches"].append(query)
        
    except Exception as e:
        st.warning(f"Organic/Ads ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    return collected_data



def extract_high_intent_keywords(client, serp_data, country, product_name):
    """
    SerpApi ë°ì´í„°ì—ì„œ ê³ ì˜ë„ ë§ˆì¼€íŒ… í‚¤ì›Œë“œ(ì˜ì–´ ì›ë³¸) ì¶”ì¶œ
    - ë¸Œëœë“œ/ë¦¬í…Œì¼ëŸ¬/ìš©ëŸ‰/ì •ë³´ì„± í‚¤ì›Œë“œ ì œì™¸
    - ë°˜í™˜: (comma-separated English string, is_fallback)
    """
    is_fallback = False

    if serp_data and any(serp_data.values()):
        context_parts = []
        if serp_data.get("shopping_titles"):
            context_parts.append(f"Shopping Titles: {serp_data['shopping_titles'][:20]}")
        if serp_data.get("ads_headlines"):
            context_parts.append(f"Paid Ads (High-Intent): {serp_data['ads_headlines'][:10]}")
        if serp_data.get("related_searches"):
            context_parts.append(f"Related Searches: {serp_data['related_searches']}")
        context = "\n".join(context_parts)
        task_desc = "Extract the top 10 high-volume commercial keywords from the above data."
    else:
        is_fallback = True
        context = "No real-time SerpApi data available."
        task_desc = f"Generate top 10 commercial keywords for '{product_name}' based on general market knowledge for {country}."

    prompt = f"""
Role: E-commerce SEO Keyword Analyst for {get_country_name(country)}.

Context:
{context}

Task: {task_desc}

â›” CRITICAL EXCLUSION RULES (MUST FOLLOW):
1. NO BRAND NAMES (Nike, Adidas, Samsung, Apple, etc.)
2. NO RETAILER/PLATFORM NAMES (Amazon, Walmart, Target, etc.)
3. NO SPECIFIC MEASUREMENTS (500g, 200ml, 12 pack, etc.)
4. NO INFORMATIONAL TERMS (recipe, how to, tutorial, history, calories, nutrition facts)

âœ… OUTPUT REQUIREMENTS:
- Output EXACTLY 10 keywords
- Output ENGLISH ONLY
- Each keyword: 1~5 words, commercial intent
- Each keyword on a NEW LINE
- No numbering, no bullet symbols, no extra text
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        raw = (response.choices[0].message.content or "").strip()

        # 1) ì•ˆì „í•œ í›„ì²˜ë¦¬: í”„ë¡¬í”„íŠ¸/í—¤ë”ê°€ ì„ì—¬ ë‚˜ì˜¤ë©´ ì œê±°
        lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
        bad_prefix = ("role:", "context:", "task:", "output", "data available:")
        cleaned = []
        for ln in lines:
            lnl = ln.lower()
            if any(lnl.startswith(p) for p in bad_prefix):
                continue
            ln = re.sub(r'^\d+[\.\)]\s*', '', ln)  # 1. ì œê±°
            ln = ln.strip("â€¢-â€“â€” \t")
            ln = ln.rstrip(",")
            if ln:
                cleaned.append(ln)

        # 2) ì¤„ì´ í•œ ì¤„ë¡œ ë¶™ì–´ì„œ ì™”ìœ¼ë©´ ì½¤ë§ˆ ê¸°ì¤€ë„ ë¶„í•´
        if len(cleaned) <= 1:
            parts = re.split(r'[,\n]+', raw)
            cleaned = [p.strip().rstrip(",") for p in parts if p.strip()]

        # 3) ê¸¸ì´/ì¤‘ë³µ ì •ë¦¬
        seen = set()
        final = []
        for kw in cleaned:
            norm = re.sub(r'\s+', ' ', kw).strip()
            if not norm:
                continue
            key = norm.lower()
            if key in seen:
                continue
            seen.add(key)
            final.append(norm)
            if len(final) >= 10:
                break

        if not final:
            return "ë¶„ì„ ì‹¤íŒ¨", True

        return ", ".join(final), is_fallback

    except Exception as e:
        st.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return "ë¶„ì„ ì‹¤íŒ¨", True

def extract_longtail_keywords(client, serp_data, country, product_name):
    """
    People Also Ask ë° Related Searchesì—ì„œ ë¡±í…Œì¼ í‚¤ì›Œë“œ(ì˜ì–´) ì¶”ì¶œ
    - ë°˜í™˜: List[str] (ìµœëŒ€ 8ê°œ)
    """
    if not serp_data:
        return []

    paa_data = serp_data.get("people_also_ask", [])
    related_data = serp_data.get("related_searches", [])

    if not paa_data and not related_data:
        return []

    prompt = f"""
Role: SEO Long-tail Keyword Specialist for {get_country_name(country)}.

Data:
- People Also Ask: {paa_data}
- Related Searches: {related_data}

Product (English): {product_name}

TASK:
Generate EXACTLY 8 long-tail keywords (3-6 words) in ENGLISH.
They should include:
- informational intent
- comparison intent
- purchase intent

â›” Exclude brand names, retailer/platform names, and specific measurements.
âœ… Output rules:
- ENGLISH ONLY
- Each keyword on a NEW LINE
- No numbering, no bullets, no extra text
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        raw = (response.choices[0].message.content or "").strip()

        lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
        bad_prefix = ("role:", "data:", "task:", "output", "product")
        cleaned = []
        for ln in lines:
            lnl = ln.lower()
            if any(lnl.startswith(p) for p in bad_prefix):
                continue
            ln = re.sub(r'^\d+[\.\)]\s*', '', ln)
            ln = ln.strip("â€¢-â€“â€” \t")
            ln = ln.rstrip(",")
            if ln:
                cleaned.append(ln)

        # ì½¤ë§ˆë¡œ í•œ ì¤„ì— ë­‰ì³ì˜¨ ê²½ìš° ë¶„í•´
        if len(cleaned) <= 1:
            parts = re.split(r'[,\n]+', raw)
            cleaned = [p.strip().rstrip(",") for p in parts if p.strip()]

        # ì¤‘ë³µ ì œê±° + 8ê°œ ì œí•œ
        seen = set()
        final = []
        for kw in cleaned:
            norm = re.sub(r'\s+', ' ', kw).strip()
            if not norm:
                continue
            key = norm.lower()
            if key in seen:
                continue
            seen.add(key)
            final.append(norm)
            if len(final) >= 8:
                break

        return final

    except Exception:
        return []
def fetch_google_trends(seed_keyword, geo_code):
    """Google Trends ë°ì´í„° ìˆ˜ì§‘"""
    pytrends = TrendReq(hl='en-US', tz=360)
    
    for timeframe in ['today 12-m', 'today 3-m', 'today 1-m']:
        try:
            pytrends.build_payload([seed_keyword], timeframe=timeframe, geo=geo_code)
            related = pytrends.related_queries()
            
            if related and seed_keyword in related:
                top_df = related[seed_keyword]['top']
                if top_df is not None and not top_df.empty:
                    return top_df.head(10)['query'].tolist()
            
            time.sleep(1)
        except:
            continue
    
    return []


# ============================================
# DeepL ë²ˆì—­ í•¨ìˆ˜
# ============================================

def translate_with_deepl(text, target_country):
    """
    DeepL APIë¥¼ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ ë²ˆì—­
    target_country: êµ­ê°€ ì½”ë“œ (ì˜ˆ: 'JP', 'KR', 'FR')
    """
    if not DEEPL_API_KEY:
        return None
    
    # êµ­ê°€ ì½”ë“œ â†’ DeepL ì–¸ì–´ ì½”ë“œ ë§¤í•‘
    deepl_lang_map = {
        'JP': 'JA', 'KR': 'KO', 'CN': 'ZH', 'TW': 'ZH', 'HK': 'ZH',
        'FR': 'FR', 'DE': 'DE', 'ES': 'ES', 'IT': 'IT',
        'PT': 'PT-PT', 'BR': 'PT-BR',
        'NL': 'NL', 'PL': 'PL', 'RU': 'RU', 'TR': 'TR',
        'SE': 'SV', 'NO': 'NB', 'DK': 'DA', 'FI': 'FI',
        'GR': 'EL', 'CZ': 'CS', 'RO': 'RO', 'HU': 'HU',
        'ID': 'ID', 'AR': 'AR', 'TH': 'TH', 'VI': 'VI'
    }
    
    target_lang = deepl_lang_map.get(target_country)
    if not target_lang:
        return None  # DeepLì´ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´
    
    api_url = "https://api-free.deepl.com/v2/translate"
    
    try:
        response = requests.post(
            api_url,
            data={
                'auth_key': DEEPL_API_KEY,
                'text': text,
                'target_lang': target_lang,
                'source_lang': 'EN'  # ì†ŒìŠ¤ëŠ” ì˜ì–´ë¡œ ê³ ì •
            },
            timeout=15
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['translations'][0]['text']
        else:
            return None
    except:
        return None

def translate_with_gpt(client, text, target_country):
    """
    GPTë¥¼ ì‚¬ìš©í•œ ë²ˆì—­ (DeepL ì‹¤íŒ¨ ì‹œ í´ë°±)
    """
    lang_name = get_language_name(target_country)

    prompt = f"""Translate the following English text to {lang_name}.
Keep the marketing tone and style.
Do NOT add any explanations, just provide the translated text.

Text to translate:
{text}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except:
        return None



def translate_to_english_with_deepl(text, source_country):
    """
    DeepLë¡œ (í˜„ì§€ì–´ -> ì˜ì–´) ë²ˆì—­. ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ None.
    - DeepLì€ ì–¸ì–´ ìë™ê°ì§€ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ source_langì„ ì§€ì •í•˜ì§€ ì•ŠìŒ
    """
    if not DEEPL_API_KEY:
        return None

    # DeepLì´ ì§€ì›í•˜ëŠ” íƒ€ê²Ÿ êµ­ê°€ë§Œ ë¹ ë¥´ê²Œ ì²´í¬ (ê¸°ì¡´ ë§µ ì¬ì‚¬ìš©)
    deepl_supported = {
        'JP','KR','CN','TW','HK','FR','DE','ES','IT','PT','BR','NL','PL','RU','TR',
        'SE','NO','DK','FI','GR','CZ','RO','HU','ID','AR','TH','VI'
    }
    if source_country not in deepl_supported:
        return None

    api_url = "https://api-free.deepl.com/v2/translate"
    try:
        response = requests.post(
            api_url,
            data={
                'auth_key': DEEPL_API_KEY,
                'text': text,
                'target_lang': 'EN'  # ì˜ì–´ë¡œ
            },
            timeout=15
        )
        if response.status_code == 200:
            result = response.json()
            return result['translations'][0]['text']
        return None
    except:
        return None


def translate_to_english(client, text, source_country):
    """
    (í˜„ì§€ì–´ -> ì˜ì–´) ë²ˆì—­. DeepL ìš°ì„ , ì‹¤íŒ¨ ì‹œ GPT í´ë°±.
    - ì¶œë ¥ì€ 'ì˜ì–´ë§Œ' ë‚˜ì˜¤ë„ë¡ ê°•ì œ
    """
    if not text:
        return ""

    # 1) DeepL ìš°ì„ 
    en = translate_to_english_with_deepl(text, source_country)
    if en:
        return en.strip()

    # 2) GPT í´ë°±
    lang_name = get_language_name(source_country)
    prompt = f"""Translate the following text from {lang_name} to English.
Rules:
- Output ENGLISH ONLY.
- Keep the original meaning and marketing tone.
- Do NOT add explanations, labels, or extra commentary.

Text:
{text}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2
        )
        en = (response.choices[0].message.content or "").strip()

        # ì˜ì–´ë§Œ ë‚˜ì˜¤ë„ë¡ 1íšŒ í›„ì²˜ë¦¬: ë¹„ë¼í‹´ ë¬¸ìê°€ ë§ì´ ì„ì´ë©´ ì¬ìš”ì²­
        non_latin = sum(1 for ch in en if ord(ch) > 127)
        if len(en) > 0 and (non_latin / max(len(en), 1)) > 0.15:
            retry_prompt = f"""Rewrite the following into ENGLISH ONLY.
Do not include any non-English words.

Text:
{en}"""
            response2 = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": retry_prompt}],
                temperature=0.1
            )
            en = (response2.choices[0].message.content or "").strip()

        return en
    except:
        return None


def clean_trends_keywords(client, keywords, product_en, country):
    """
    Google Trends ê²°ê³¼ì—ì„œ ë¸Œëœë“œ/ë¦¬í…Œì¼ëŸ¬/ìš©ëŸ‰ ë“±ì„ ì œê±°í•˜ê³  10ê°œë¡œ ì •ë¦¬.
    - ì…ë ¥ ì–¸ì–´ë¥¼ ìœ ì§€(í˜„ì§€ì–´ë©´ í˜„ì§€ì–´ë¡œ ìœ ì§€)
    """
    if not keywords:
        return []

    prompt = f"""
You are a strict SEO keyword cleaner for {get_country_name(country)}.

Given this list of search queries:
{keywords}

Remove:
- brand names (any company/brand)
- retailer/platform names (Amazon, etc.)
- specific measurements (ml, g, oz, pack, etc.)
- overly specific model numbers

Keep:
- generic, commercial, high-intent queries

Return up to 10 cleaned queries.
OUTPUT RULES:
- Return ONLY the cleaned queries
- One per line
- Keep the SAME LANGUAGE as the input list
"""
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2
        )
        raw = (resp.choices[0].message.content or "").strip()
        lines = [ln.strip().strip("â€¢-â€“â€” \t").rstrip(",") for ln in raw.splitlines() if ln.strip()]
        # í—¤ë” ì œê±°
        lines = [ln for ln in lines if not ln.lower().startswith(("output", "return", "cleaned"))]
        # ì¤‘ë³µ ì œê±°
        seen = set()
        final = []
        for ln in lines:
            key = ln.lower()
            if key in seen:
                continue
            seen.add(key)
            final.append(ln)
            if len(final) >= 10:
                break
        return final if final else keywords[:10]
    except:
        return keywords[:10]


# ============================================
# íƒ€ê²Ÿ ì†Œë¹„ì¸µ ë¶„ì„ (Segmentation)
# ============================================

def generate_target_audience_analysis(client, keywords, product, country, serp_data):
    """
    íƒ€ê²Ÿ ì†Œë¹„ì¸µ ë¶„ì„ (Demographics & Persona Summary ìœ„ì£¼ë¡œ ì••ì¶•)
    """
    country_name = get_country_name(country)
    
    # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
    context_parts = [f"Product: {product}", f"High-Intent Keywords: {keywords}"]
    if serp_data and serp_data.get("ads_headlines"):
        context_parts.append(f"Competitor Ads: {serp_data['ads_headlines'][:3]}")
    
    context = "\n".join(context_parts)
    
    prompt = f"""
You are a Market Segmentation Analyst for {country_name}.

CONTEXT:
{context}

TASK: Provide a CONCISE Demographic & Persona analysis. Do NOT write long paragraphs. Use bullet points.

OUTPUT FORMAT:

### Core Demographics
* **Age Group:** [e.g., 25-34]
* **Gender:** [e.g., Female skewing]
* **Income Level:** [e.g., Upper-middle class]
* **Location:** [e.g., Urban areas]
* **Occupation:** [Key job titles]

### Psychographics & Pain Points
* **Core Values:** [3 key values]
* **Main Pain Point:** [The #1 problem they want to solve]
* **Buying Motivation:** [Why they buy this specific product]

### Quick Marketing Insight
* [One sentence on the best angle to approach this audience]

Write in English. Keep it brief and actionable.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"íƒ€ê²Ÿ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

def generate_high_quality_content(client, target_analysis, keywords, product, country):
    """
    íƒ€ê²Ÿ ì†Œë¹„ì¸µ ê¸°ë°˜ ê³ í’ˆì§ˆ ë§ˆì¼€íŒ… ì½˜í…ì¸  ìƒì„± (í•­ìƒ ì˜ì–´ë¡œ ìƒì„±)
    """
    country_name = get_country_name(country)
    
    prompt = f"""
You are an Elite E-commerce Copywriter for the {country_name} market.
You specialize in high-conversion sales copy that drives immediate purchases.

TARGET AUDIENCE INSIGHTS:
{target_analysis}

Product: {product}
Keywords to Integrate: {keywords}

=== CRITICAL LANGUAGE INSTRUCTION ===
- Write ENTIRELY in ENGLISH (it will be translated later if needed)
- Use culturally appropriate expressions for {country_name} market
- All keywords, descriptions, and hashtags MUST be in ENGLISH

TASK: Write 3 types of marketing assets with a "Premium Brand" tone.

=== 1. AMAZON BULLET POINTS (5 Bullets) ===
Style Guide:
- **Format:** [BENEFIT HEADER IN CAPS] - [Detailed explanation]
- **Header:** 2-4 words, punchy, benefit-driven (e.g., "INSTANT PAIN RELIEF", "ULTRA-DURABLE MATERIAL")
- **Body:** 2-3 sentences. Explain WHY this matters. Use sensory words.
- **Goal:** Overcome objections and create desire.
- **Length:** 200-250 characters per bullet (optimal for mobile indexing).

=== 2. D2C PRODUCT DESCRIPTION (Storytelling) ===
Style Guide:
- **Tone:** Empathetic, authoritative, yet accessible.
- **Structure:**
  1. **Headline:** A promise of transformation (e.g., "Reclaim Your Comfort Today")
  2. **The Problem:** Empathize with the user's struggle (1 paragraph)
  3. **The Solution:** Introduce the product as the hero (1 paragraph)
  4. **The "Why Us":** Highlight unique selling points/technology (1 paragraph)
  5. **Closing:** A strong call to action.

=== 3. SOCIAL MEDIA POST (Instagram/Facebook) ===
Style Guide:
- **Hook:** A question or bold statement to stop the scroll.
- **Vibe:** Lifestyle-focused, not just product specs.
- **Structure:** Hook -> Relatable Scenario -> Product Benefit -> CTA.
- **Emojis:** Use relevant emojis to break up text (but don't overdo it).
- **Hashtags:** Mix of broad (#Marketing) and niche (#EcoFriendlyLiving) tags.

=== OUTPUT FORMAT (Strictly follow this) ===

---AMAZON_START---
- [HEADER]: [Content]
- [HEADER]: [Content]
- [HEADER]: [Content]
- [HEADER]: [Content]
- [HEADER]: [Content]
---AMAZON_END---

---D2C_START---
# [Headline]

[Body Paragraph 1 - Problem]

[Body Paragraph 2 - Solution]

[Body Paragraph 3 - Features]

[CTA]
---D2C_END---

---SOCIAL_START---
[Hook]

[Body]

[CTA]

[Hashtags]
---SOCIAL_END---

IMPORTANT: Write EVERYTHING in ENGLISH.
Ensure keywords are woven in naturally for SEO.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.75
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨: {str(e)}"


def parse_persona_content(content_text):
    """
    í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì½˜í…ì¸ ë¥¼ ì„¹ì…˜ë³„ë¡œ íŒŒì‹±
    Returns: (amazon_text, d2c_text, social_text)
    """
    import re
    
    # ê¸°ë³¸ê°’
    amazon_text = ""
    d2c_text = ""
    social_text = ""
    
    try:
        # Amazon ì„¹ì…˜ ì¶”ì¶œ
        amazon_match = re.search(r'---AMAZON_START---(.*?)---AMAZON_END---', content_text, re.DOTALL)
        if amazon_match:
            amazon_text = amazon_match.group(1).strip()
        
        # D2C ì„¹ì…˜ ì¶”ì¶œ
        d2c_match = re.search(r'---D2C_START---(.*?)---D2C_END---', content_text, re.DOTALL)
        if d2c_match:
            d2c_text = d2c_match.group(1).strip()
        
        # Social ì„¹ì…˜ ì¶”ì¶œ
        social_match = re.search(r'---SOCIAL_START---(.*?)---SOCIAL_END---', content_text, re.DOTALL)
        if social_match:
            social_text = social_match.group(1).strip()
        
        # ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš° (fallback) - # ê¸°ì¤€ìœ¼ë¡œ íŒŒì‹±
        if not amazon_text or not d2c_text or not social_text:
            sections = content_text.split('#')
            
            for section in sections:
                section_lower = section.lower()
                
                if 'amazon' in section_lower and not amazon_text:
                    # "Amazon Bullet Points" í—¤ë” ì œê±°
                    lines = section.split('\n')
                    content_lines = [line for line in lines if 'amazon' not in line.lower() and line.strip()]
                    amazon_text = '\n'.join(content_lines).strip()
                
                elif ('d2c' in section_lower or 'product description' in section_lower) and not d2c_text:
                    lines = section.split('\n')
                    content_lines = [line for line in lines if 'd2c' not in line.lower() and 'product description' not in line.lower() and line.strip()]
                    d2c_text = '\n'.join(content_lines).strip()
                
                elif ('social' in section_lower or 'instagram' in section_lower or 'facebook' in section_lower) and not social_text:
                    lines = section.split('\n')
                    content_lines = [line for line in lines if 'social' not in line.lower() and 'instagram' not in line.lower() and 'facebook' not in line.lower() and line.strip()]
                    social_text = '\n'.join(content_lines).strip()
        
        # ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°ê°ì— í• ë‹¹ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        if not amazon_text:
            amazon_text = "ì½˜í…ì¸  íŒŒì‹± ì‹¤íŒ¨ - ì›ë³¸ ì „ì²´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
        if not d2c_text:
            d2c_text = "ì½˜í…ì¸  íŒŒì‹± ì‹¤íŒ¨ - ì›ë³¸ ì „ì²´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
        if not social_text:
            social_text = "ì½˜í…ì¸  íŒŒì‹± ì‹¤íŒ¨ - ì›ë³¸ ì „ì²´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
        
    except Exception as e:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        error_msg = f"íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
        return error_msg, error_msg, error_msg
    
    return amazon_text, d2c_text, social_text


# ============================================
# ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ============================================

# 1. ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™” (ë°ì´í„° ìœ ì§€ìš©)
if 'analysis_result' not in st.session_state:
    st.session_state.analysis_result = None

# âœ… ì…ë ¥ê°’ ë³€ê²½ ì‹œ í¸ì§‘ ìœ„ì ¯(text_area) ìƒíƒœê°€ ì´ì „ ê°’ì„ ëŒê³  ì˜¤ëŠ” ë¬¸ì œ ë°©ì§€
if 'last_run_signature' not in st.session_state:
    st.session_state.last_run_signature = None

def reset_output_widgets():
    # Streamlit text_areaëŠ” keyê°€ ê°™ìœ¼ë©´ ì´ì „ ê°’(session_state)ì„ ê³„ì† ìœ ì§€í•˜ë¯€ë¡œ ì‚­ì œ í•„ìš”
    for k in ("amazon_edit_area", "d2c_edit_area", "sns_edit_area"):
        if k in st.session_state:
            del st.session_state[k]

# âœ… ì…ë ¥(ì œí’ˆ/êµ­ê°€/ì…ë ¥ë°©ì‹) ë³€ê²½ ì‹œ ì´ì „ ë¶„ì„ ê²°ê³¼/í¸ì§‘ìƒíƒœ ìë™ ì´ˆê¸°í™” (ë¸Œë¼ìš°ì € ìƒˆë¡œê³ ì¹¨ íš¨ê³¼)
current_input_signature = f"{input_type}|{user_input}|{target_country}"
if 'last_input_signature' not in st.session_state:
    st.session_state.last_input_signature = current_input_signature
elif st.session_state.last_input_signature != current_input_signature:
    # ì‚¬ìš©ìê°€ ì œí’ˆ/êµ­ê°€ ë“±ì„ ë°”ê¾¼ ìƒíƒœì—ì„œ ì´ì „ ê²°ê³¼ê°€ í™”ë©´ì— ë‚¨ì•„ í˜¼ë™ë˜ëŠ” ê²ƒì„ ë°©ì§€
    reset_output_widgets()
    st.session_state.analysis_result = None
    st.session_state.last_input_signature = current_input_signature

# 2. ë¶„ì„ ë²„íŠ¼ ì‹¤í–‰ ë¡œì§
if analyze_btn:
    # â˜…â˜…â˜… ìƒˆ ë¶„ì„ ì‹œì‘ ì‹œ ì´ì „ ì„¸ì…˜/ìœ„ì ¯ ì´ˆê¸°í™” â˜…â˜…â˜…
    reset_output_widgets()
    st.session_state.last_run_signature = f"{input_type}|{user_input}|{target_country}"
    st.session_state.analysis_result = None
    
    if not user_input or not target_country:
        st.warning("âš ï¸ ì œí’ˆëª…/HS Codeì™€ íƒ€ê²Ÿ êµ­ê°€ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not OPENAI_API_KEY:
        st.error("âŒ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = OpenAI(api_key=OPENAI_API_KEY)
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # --- [Step 1~4: ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„] ---
        
        status_text.text("ğŸ” Step 1/7: ì œí’ˆëª… ì‹ë³„ ì¤‘...")
        progress_bar.progress(10)
        native_kw, english_kw = get_seed_keyword(client, user_input, input_type, target_country)
        
        if not native_kw:
            st.error("ì‹ë³„ ì‹¤íŒ¨")
            st.stop()

        # ì–¸ì–´ ë³´ì •
        if target_country in ['US', 'GB', 'AU', 'CA', 'NZ', 'SG', 'IE', 'ZA', 'NG', 'PH', 'IN'] and any(ord(c) > 127 for c in native_kw):
            native_kw = english_kw

        status_text.text("ğŸŒ Step 2/7: ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ...")
        progress_bar.progress(30)
        serp_data = fetch_comprehensive_serpapi_data(native_kw, target_country)

        status_text.text("ğŸ’ Step 3/7: ê³ ì˜ë„ í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
        high_intent_kw, is_fallback = extract_high_intent_keywords(client, serp_data, target_country, english_kw)
        longtail_kw = extract_longtail_keywords(client, serp_data, target_country, english_kw)
        progress_bar.progress(45)

        # í‚¤ì›Œë“œ í˜„ì§€í™” ì²˜ë¦¬
        is_english_country = target_country in ENGLISH_COUNTRIES

        # (ì˜ì–´ ì›ë³¸ì„ ë³„ë„ë¡œ ë³´ê´€) - ë¹„ì˜ì–´ê¶Œì—ì„œ 'í˜„ì§€ì–´ + ì˜ì–´' í•¨ê»˜ í‘œì‹œí•˜ê¸° ìœ„í•¨
        high_intent_kw_en_list = []
        high_intent_kw_local_list = []
        longtail_kw_en = list(longtail_kw) if longtail_kw else []
        longtail_kw_local = list(longtail_kw) if longtail_kw else []

        if high_intent_kw and "ë¶„ì„ ì‹¤íŒ¨" not in high_intent_kw:
            high_intent_kw_en_list = [k.strip() for k in high_intent_kw.split(',') if k.strip()]

        if not is_english_country:
            status_text.text("ğŸŒ Step 3/7: í‚¤ì›Œë“œ í˜„ì§€í™” ì¤‘...")

            # High-Intent í‚¤ì›Œë“œ: 'ì˜ì–´ ë¦¬ìŠ¤íŠ¸'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°œë³„ ë²ˆì—­í•˜ì—¬ ì •ë ¬/ì˜ë¯¸ ë³´ì¡´
            if high_intent_kw_en_list:
                for kw in high_intent_kw_en_list:
                    translated = translate_with_deepl(kw, target_country)
                    if not translated:
                        translated = translate_with_gpt(client, kw, target_country)
                    high_intent_kw_local_list.append(translated if translated else kw)
                    time.sleep(0.1)

                # ê¸°ì¡´ í˜¸í™˜ìš©(ë¬¸ìì—´)ë„ ìœ ì§€
                high_intent_kw = ", ".join(high_intent_kw_local_list)

            # Long-tail í‚¤ì›Œë“œ: ì˜ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°œë³„ ë²ˆì—­
            if longtail_kw_en:
                translated_longtail = []
                for kw in longtail_kw_en:
                    translated = translate_with_deepl(kw, target_country)
                    if not translated:
                        translated = translate_with_gpt(client, kw, target_country)
                    translated_longtail.append(translated if translated else kw)
                    time.sleep(0.1)
                longtail_kw_local = translated_longtail
                longtail_kw = longtail_kw_local
        else:
            # ì˜ì–´ê¶Œ: í˜„ì§€ì–´=ì˜ì–´
            high_intent_kw_local_list = list(high_intent_kw_en_list)
            longtail_kw_local = list(longtail_kw_en)

        progress_bar.progress(50)

        status_text.text("ğŸ“ˆ Step 4/7: Google Trends ë¶„ì„ ì¤‘...")
        trends_kw_raw = fetch_google_trends(native_kw, target_country)
        # âœ… ë¸Œëœë“œ/ë¦¬í…Œì¼ëŸ¬/ìš©ëŸ‰ ì œê±° (TrendsëŠ” ì›ì²œ ë°ì´í„°ë¼ í•„í„°ê°€ í•„ìš”)
        trends_kw = clean_trends_keywords(client, trends_kw_raw, english_kw, target_country)
        progress_bar.progress(60)

        # Trends í‚¤ì›Œë“œ ì˜ì–´ ì˜ë¯¸(ë¹„ì˜ì–´ê¶Œ ëŒ€ì‹œë³´ë“œ í‘œì‹œìš©)
        trends_kw_en = []
        if target_country not in ENGLISH_COUNTRIES and trends_kw:
            for kw in trends_kw:
                en = translate_to_english(client, kw, target_country)
                trends_kw_en.append(en if en else kw)
                time.sleep(0.1)
        else:
            trends_kw_en = list(trends_kw) if trends_kw else []

        status_text.text("ğŸ‘¥ Step 5/7: íƒ€ê²Ÿ ì†Œë¹„ì¸µ ë¶„ì„ ì¤‘...")
        # âœ… ë¶„ì„/ì½˜í…ì¸  ìƒì„±ìš© í‚¤ì›Œë“œëŠ” í•­ìƒ ì˜ì–´ ì›ë³¸ ì‚¬ìš© (í˜¼ìš©/ë²ˆì—­ ì˜¤ë¥˜ ë°©ì§€)
        high_intent_kw_for_generation = ", ".join(high_intent_kw_en_list) if high_intent_kw_en_list else english_kw
        target_analysis = generate_target_audience_analysis(client, high_intent_kw_for_generation, english_kw, target_country, serp_data)
        progress_bar.progress(70)

        status_text.text("âœï¸ Step 6/7: ê³ í’ˆì§ˆ ë§ˆì¼€íŒ… ì½˜í…ì¸  ìƒì„± ì¤‘...")
        marketing_content_en = generate_high_quality_content(client, target_analysis, high_intent_kw_for_generation, english_kw, target_country)
        amazon_en, d2c_en, social_en = parse_persona_content(marketing_content_en)
        progress_bar.progress(80)

        status_text.text("ğŸŒ Step 7/7: í˜„ì§€í™” ë²ˆì—­ ì¤‘...")
        
        # ì˜ì–´ê¶Œ ì—¬ë¶€ í™•ì¸
        is_english_country = target_country in ['US', 'GB', 'AU', 'CA', 'NZ', 'SG', 'IE', 'ZA', 'NG', 'PH', 'IN']
        
        if is_english_country:
            # ì˜ì–´ê¶Œ - ë²ˆì—­ ë¶ˆí•„ìš”
            amazon_final = amazon_en
            d2c_final = d2c_en
            social_final = social_en
            translation_status = "ì˜ì–´ ì›ë³¸"
        else:
            # ë¹„ì˜ì–´ê¶Œ - DeepL ìš°ì„ , ì‹¤íŒ¨ ì‹œ GPT
            translation_results = []
            
            # ì•„ë§ˆì¡´ ë²ˆì—­
            amazon_final = translate_with_deepl(amazon_en, target_country)
            if amazon_final:
                translation_results.append("DeepL")
            else:
                amazon_final = translate_with_gpt(client, amazon_en, target_country)
                if amazon_final:
                    translation_results.append("GPT")
                else:
                    amazon_final = amazon_en
                    translation_results.append("ì‹¤íŒ¨")
            
            # ìì‚¬ëª° ë²ˆì—­
            d2c_final = translate_with_deepl(d2c_en, target_country)
            if d2c_final:
                translation_results.append("DeepL")
            else:
                d2c_final = translate_with_gpt(client, d2c_en, target_country)
                if d2c_final:
                    translation_results.append("GPT")
                else:
                    d2c_final = d2c_en
                    translation_results.append("ì‹¤íŒ¨")
            
            # SNS ë²ˆì—­
            social_final = translate_with_deepl(social_en, target_country)
            if social_final:
                translation_results.append("DeepL")
            else:
                social_final = translate_with_gpt(client, social_en, target_country)
                if social_final:
                    translation_results.append("GPT")
                else:
                    social_final = social_en
                    translation_results.append("ì‹¤íŒ¨")
            
            # ë²ˆì—­ ìƒíƒœ ê²°ì • (ì›Œë“œ íŒŒì¼ìš©ë§Œ)
            deepl_count = translation_results.count("DeepL")
            gpt_count = translation_results.count("GPT")
            
            if deepl_count == 3:
                translation_status = "DeepL"
            elif gpt_count == 3:
                translation_status = "GPT"
            elif deepl_count + gpt_count == 3:
                translation_status = "DeepL + GPT"
            else:
                translation_status = "ë¶€ë¶„ ë²ˆì—­"

        # âœ… ë¹„ì˜ì–´ê¶Œ: í˜„ì§€ì–´ ìµœì¢…ë³¸ì˜ 'ì˜ì–´ ì˜ë¯¸'ë¥¼ ë³„ë„ë¡œ ìƒì„±(í˜¼ìš©/ê¹¨ì§ ë°©ì§€)
        amazon_meaning_en = None
        d2c_meaning_en = None
        social_meaning_en = None
        if not is_english_country:
            amazon_meaning_en = translate_to_english(client, amazon_final, target_country) or amazon_en
            d2c_meaning_en = translate_to_english(client, d2c_final, target_country) or d2c_en
            social_meaning_en = translate_to_english(client, social_final, target_country) or social_en

        progress_bar.progress(100)
        time.sleep(0.5)
        status_text.empty()
        progress_bar.empty()

        # ê²°ê³¼ ì„¸ì…˜ì— ì €ì¥ (í™”ë©´ ë¦¬í”„ë ˆì‹œ ë˜ë”ë¼ë„ ìœ ì§€)
        st.session_state.analysis_result = {
            'native_kw': native_kw,
            'english_kw': english_kw,
            'target_country': target_country,
            'high_intent_kw': high_intent_kw,
            'high_intent_kw_en_list': high_intent_kw_en_list,
            'high_intent_kw_local_list': high_intent_kw_local_list,
            'longtail_kw': longtail_kw,
            'longtail_kw_en': longtail_kw_en,
            'longtail_kw_local': longtail_kw_local,
            'trends_kw': trends_kw,
            'trends_kw_en': trends_kw_en,
            'target_analysis': target_analysis,
            'amazon_en': amazon_en,
            'd2c_en': d2c_en,
            'social_en': social_en,
            'amazon_meaning_en': amazon_meaning_en,
            'd2c_meaning_en': d2c_meaning_en,
            'social_meaning_en': social_meaning_en,
            'amazon_final': amazon_final,
            'd2c_final': d2c_final,
            'social_final': social_final,
            'translation_status': translation_status,
            'is_fallback': is_fallback,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }

# 3. ê²°ê³¼ í™”ë©´ ì¶œë ¥ (ì„¸ì…˜ì— ë°ì´í„°ê°€ ìˆì„ ê²½ìš° í•­ìƒ í‘œì‹œ)
if st.session_state.analysis_result:
    data = st.session_state.analysis_result
    saved_country = data['target_country']
    
    # â˜…â˜…â˜… ë²ˆì—­ ìƒíƒœ ë©”ì‹œì§€ ì œê±° - ì„±ê³µ ë©”ì‹œì§€ë§Œ í‘œì‹œ â˜…â˜…â˜…
    st.success(f"âœ… **{get_country_name(saved_country)}** ì‹œì¥ ë¶„ì„ ì™„ë£Œ: **{data['english_kw']}** ({data['native_kw']})")

    st.divider()

    # --- íƒ€ê²Ÿ ì†Œë¹„ì¸µ ë¶„ì„ ì„¹ì…˜ ---
    st.header("íƒ€ê²Ÿ ì†Œë¹„ì¸µ ë¶„ì„ (Demographics)")
    with st.expander("ğŸ“Š í•µì‹¬ ì¸êµ¬í†µê³„ ë° ì¸ì‚¬ì´íŠ¸ ë³´ê¸°", expanded=True):
        st.markdown(data['target_analysis'])

    st.divider()

    # --- í‚¤ì›Œë“œ ì„¹ì…˜ ---
    st.header("ì¶”ì¶œëœ í‚¤ì›Œë“œ")
    k_tab1, k_tab2, k_tab3 = st.tabs(["ğŸ’ High-Intent", "ğŸ” Long-tail", "ğŸ“ˆ Trends"])
    
    with k_tab1:
        st.subheader("ê³ ì˜ë„ ë§ˆì¼€íŒ… í‚¤ì›Œë“œ")
        st.caption("ë¸Œëœë“œëª…Â·ìš©ëŸ‰ ì œì™¸, ì „í™˜ìœ¨ ë†’ì€ í‚¤ì›Œë“œ")
        
        if "ë¶„ì„ ì‹¤íŒ¨" not in data['high_intent_kw']:
            # â˜…â˜…â˜… ì•„ëì–´/íˆë¸Œë¦¬ì–´ ë“± RTL ì–¸ì–´ ì²˜ë¦¬ ê°œì„  â˜…â˜…â˜…
            is_english_country_ui = saved_country in ENGLISH_COUNTRIES
            
            if (not is_english_country_ui) and data.get('high_intent_kw_local_list') and data.get('high_intent_kw_en_list'):
                # í˜„ì§€ì–´ + ì˜ì–´(íšŒìƒ‰) í•¨ê»˜ í‘œì‹œ
                pairs = list(zip(data['high_intent_kw_local_list'], data['high_intent_kw_en_list']))
                tags_html = "".join([
                    f'<span class="keyword-tag">ğŸ”¥ {html.escape(local)}<span class="keyword-eng">{html.escape(en)}</span></span>'
                    for local, en in pairs
                ])
                st.markdown(tags_html, unsafe_allow_html=True)
            else:
                keywords_list = [k.strip() for k in data['high_intent_kw'].split(',') if k.strip()]
                tags_html = "".join([f'<span class="keyword-tag">ğŸ”¥ {html.escape(kw)}</span>' for kw in keywords_list])
                st.markdown(tags_html, unsafe_allow_html=True)
        else:
            st.error("í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")

    with k_tab2:
        st.subheader("ğŸ” ë¡±í…Œì¼ í‚¤ì›Œë“œ (SEO ìµœì í™”)")
        st.caption("'People Also Ask' ë° 'ê´€ë ¨ ê²€ìƒ‰ì–´' ê¸°ë°˜")
        
        if data['longtail_kw']:
            is_english_country_ui = saved_country in ENGLISH_COUNTRIES
            
            if (not is_english_country_ui) and data.get('longtail_kw_local') and data.get('longtail_kw_en'):
                for idx, (local_kw, en_kw) in enumerate(zip(data['longtail_kw_local'], data['longtail_kw_en']), 1):
                    st.markdown(f"**{idx}.** {local_kw}")
                    st.markdown(f"<div style='color:#8a8f98;font-size:12px;margin-left:22px;'>{html.escape(en_kw)}</div>", unsafe_allow_html=True)
            else:
                for idx, kw in enumerate(data['longtail_kw'], 1):
                    st.markdown(f"**{idx}.** {kw}")
        else:
            st.warning("ë°ì´í„° ë¶€ì¡±")

    with k_tab3:
        st.subheader("ğŸ“ˆ Google Trends ì¸ê¸° ê²€ìƒ‰ì–´")
        st.caption("ì‹¤ì œ ì†Œë¹„ì ê²€ìƒ‰ í–‰ë™ ë°ì´í„°")
        
        if data['trends_kw']:
            is_english_country_ui = saved_country in ENGLISH_COUNTRIES
            
            if (not is_english_country_ui) and data.get('trends_kw_en') and data.get('trends_kw'):
                for idx, (local_kw, en_kw) in enumerate(zip(data['trends_kw'], data['trends_kw_en']), 1):
                    st.markdown(f"**{idx}.** {local_kw}")
                    st.markdown(f"<div style='color:#8a8f98;font-size:12px;margin-left:22px;'>{html.escape(en_kw)}</div>", unsafe_allow_html=True)
            else:
                for idx, kw in enumerate(data['trends_kw'], 1):
                    st.markdown(f"**{idx}.** {kw}")
        else:
            st.warning("ë°ì´í„° ë¶€ì¡±")

    st.divider()

    # --- ë§ˆì¼€íŒ… ì½˜í…ì¸  ì„¹ì…˜ ---
    st.header("ë§ˆì¼€íŒ… ì½˜í…ì¸  ìƒì„±")

    content_tab1, content_tab2, content_tab3 = st.tabs([
        "ğŸ›’ ì•„ë§ˆì¡´ ë¶ˆë ›í¬ì¸íŠ¸",
        "ğŸŒ ìì‚¬ëª° ìƒì„¸í˜ì´ì§€",
        "ğŸ“± SNS ë§ˆì¼€íŒ… í”¼ë“œ"
    ])

    with content_tab1:
        with st.container(border=True):
            st.subheader("ì•„ë§ˆì¡´ ì œí’ˆ ì„¤ëª…")
            edited_amazon = st.text_area(
                "ì•„ë§ˆì¡´ ë¶ˆë ›í¬ì¸íŠ¸ ìˆ˜ì •",
                value=data['amazon_final'],
                height=400,
                key="amazon_edit_area",
                label_visibility="collapsed"
            )

            is_english_country_ui = saved_country in ENGLISH_COUNTRIES
            if (not is_english_country_ui) and data.get('amazon_meaning_en'):
                st.markdown(
                    f"<div style='color:#8a8f98;font-size:12px; margin-top:10px; white-space:pre-wrap;'><b>English meaning</b><br>{html.escape(data['amazon_meaning_en'])}</div>",
                    unsafe_allow_html=True
                )

    with content_tab2:
        with st.container(border=True):
            st.subheader("ğŸŒ ìì‚¬ëª° ì œí’ˆ ì„¤ëª…")
            edited_d2c = st.text_area(
                "ìì‚¬ëª° ì„¤ëª… ìˆ˜ì •",
                value=data['d2c_final'],
                height=500,
                key="d2c_edit_area",
                label_visibility="collapsed"
            )

            is_english_country_ui = saved_country in ENGLISH_COUNTRIES
            if (not is_english_country_ui) and data.get('d2c_meaning_en'):
                st.markdown(
                    f"<div style='color:#8a8f98;font-size:12px; margin-top:10px; white-space:pre-wrap;'><b>English meaning</b><br>{html.escape(data['d2c_meaning_en'])}</div>",
                    unsafe_allow_html=True
                )

    with content_tab3:
        with st.container(border=True):
            st.subheader("SNS ë§ˆì¼€íŒ… í¬ìŠ¤íŠ¸")
            edited_sns = st.text_area(
                "SNS í”¼ë“œ ìˆ˜ì •",
                value=data['social_final'],
                height=400,
                key="sns_edit_area",
                label_visibility="collapsed"
            )

            is_english_country_ui = saved_country in ENGLISH_COUNTRIES
            if (not is_english_country_ui) and data.get('social_meaning_en'):
                st.markdown(
                    f"<div style='color:#8a8f98;font-size:12px; margin-top:10px; white-space:pre-wrap;'><b>English meaning</b><br>{html.escape(data['social_meaning_en'])}</div>",
                    unsafe_allow_html=True
                )

    st.divider()

    # --- ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ---
    st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ")

    # ì—‘ì…€ íŒŒì¼ ìƒì„±
    def create_excel_report_dynamic(amazon_txt, d2c_txt, sns_txt):
        output = BytesIO()
        is_english_country_dl = saved_country in ENGLISH_COUNTRIES

        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # 1. ìš”ì•½ (ì›Œë“œ íŒŒì¼ì˜ 'ì œí’ˆ ì •ë³´'ì™€ ë™ì¼í•˜ê²Œ í™•ì¥)
            summary_items = [
                ('ì œí’ˆëª… (í˜„ì§€ì–´)', data.get('native_kw', '')),
                ('ì œí’ˆëª… (ì˜ì–´)', data.get('english_kw', '')),
                ('íƒ€ê²Ÿ êµ­ê°€', get_country_name(saved_country)),
                ('ì–¸ì–´', get_language_name(saved_country)),
                ('ë²ˆì—­ ë°©ì‹', data.get('translation_status', '')),
                ('ìƒì„± ì¼ì‹œ', data.get('timestamp', ''))
            ]
            pd.DataFrame({
                'í•­ëª©': [x[0] for x in summary_items],
                'ë‚´ìš©': [x[1] for x in summary_items]
            }).to_excel(writer, sheet_name='ğŸ“‹ ìš”ì•½', index=False)

            # 2. íƒ€ê²Ÿ ë¶„ì„
            pd.DataFrame({'ë‚´ìš©': [data.get('target_analysis', '')]}).to_excel(writer, sheet_name='ğŸ¯ íƒ€ê²Ÿë¶„ì„', index=False)

            # 3. ê³ ì˜ë„ í‚¤ì›Œë“œ (+ ì˜ì–´ ì˜ë¯¸: ë¹„ì˜ì–´ê¶Œë§Œ)
            if (not is_english_country_dl) and data.get('high_intent_kw_local_list') and data.get('high_intent_kw_en_list'):
                df_kw = pd.DataFrame({
                    'í˜„ì§€ì–´ í‚¤ì›Œë“œ': data['high_intent_kw_local_list'],
                    'ì˜ì–´ ì˜ë¯¸': data['high_intent_kw_en_list']
                })
            else:
                kw_list = [k.strip() for k in (data.get('high_intent_kw', '') or '').split(',') if k.strip()]
                df_kw = pd.DataFrame({'í‚¤ì›Œë“œ': kw_list})
            df_kw.to_excel(writer, sheet_name='ğŸ’ í‚¤ì›Œë“œ', index=False)

            # 4. ì½˜í…ì¸  (+ ì˜ì–´ ì˜ë¯¸: ë¹„ì˜ì–´ê¶Œë§Œ)
            if not is_english_country_dl:
                pd.DataFrame({
                    'í˜„ì§€ì–´(ìµœì¢…)': [amazon_txt],
                    'ì˜ì–´ ì˜ë¯¸': [data.get('amazon_meaning_en') or data.get('amazon_en') or '']
                }).to_excel(writer, sheet_name='ğŸ›’ ì•„ë§ˆì¡´', index=False)

                pd.DataFrame({
                    'í˜„ì§€ì–´(ìµœì¢…)': [d2c_txt],
                    'ì˜ì–´ ì˜ë¯¸': [data.get('d2c_meaning_en') or data.get('d2c_en') or '']
                }).to_excel(writer, sheet_name='ğŸŒ ìì‚¬ëª°', index=False)

                pd.DataFrame({
                    'í˜„ì§€ì–´(ìµœì¢…)': [sns_txt],
                    'ì˜ì–´ ì˜ë¯¸': [data.get('social_meaning_en') or data.get('social_en') or '']
                }).to_excel(writer, sheet_name='ğŸ“± SNS', index=False)
            else:
                pd.DataFrame({'ì•„ë§ˆì¡´ ë¶ˆë ›': [amazon_txt]}).to_excel(writer, sheet_name='ğŸ›’ ì•„ë§ˆì¡´', index=False)
                pd.DataFrame({'ìì‚¬ëª° ìƒì„¸': [d2c_txt]}).to_excel(writer, sheet_name='ğŸŒ ìì‚¬ëª°', index=False)
                pd.DataFrame({'SNS í”¼ë“œ': [sns_txt]}).to_excel(writer, sheet_name='ğŸ“± SNS', index=False)

        output.seek(0)
        return output


    # ì›Œë“œ íŒŒì¼ ìƒì„±
    def create_word_report_dynamic(amazon_txt, d2c_txt, sns_txt):
        doc = Document()
        is_english_country_dl = saved_country in ENGLISH_COUNTRIES

        def clean_and_add_text(document, text_content):
            if not text_content:
                return
            lines = text_content.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                clean_line = line.replace('### ', '').replace('## ', '').replace('**', '').replace('__', '')
                clean_line = re.sub(r'[^\w\s\uAC00-\uD7A3\u0600-\u06FF.,!?%&()\-:;\'\"]', '', clean_line).strip()
                if not clean_line:
                    continue
                if line.startswith('* ') or line.startswith('- ') or line.startswith('â€¢ '):
                    clean_text = clean_line.lstrip('*-â€¢ ').strip()
                    document.add_paragraph(clean_text, style='List Bullet')
                else:
                    document.add_paragraph(clean_line)

        title = doc.add_heading('SEO ë§ˆì¼€íŒ… ë¶„ì„ ë³´ê³ ì„œ', 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        doc.add_heading('ì œí’ˆ ì •ë³´', 1)
        info_table = doc.add_table(rows=6, cols=2)
        info_table.style = 'Light Grid Accent 1'

        info_data = [
            ('ì œí’ˆëª… (í˜„ì§€ì–´)', data.get('native_kw', '')),
            ('ì œí’ˆëª… (ì˜ì–´)', data.get('english_kw', '')),
            ('íƒ€ê²Ÿ êµ­ê°€', get_country_name(saved_country)),
            ('ì–¸ì–´', get_language_name(saved_country)),
            ('ë²ˆì—­ ë°©ì‹', data.get('translation_status', '')),
            ('ìƒì„± ì¼ì‹œ', data.get('timestamp', ''))
        ]

        for idx, (label, value) in enumerate(info_data):
            info_table.rows[idx].cells[0].text = label
            info_table.rows[idx].cells[1].text = str(value)

        doc.add_paragraph()
        doc.add_heading('íƒ€ê²Ÿ ì†Œë¹„ì¸µ ë¶„ì„ (Segmentation)', 1)
        clean_and_add_text(doc, data.get('target_analysis', ''))
        doc.add_page_break()

        doc.add_heading('ê³ ì˜ë„ ë§ˆì¼€íŒ… í‚¤ì›Œë“œ', 1)
        doc.add_paragraph('ë¸Œëœë“œëª…Â·ìš©ëŸ‰ ì œì™¸, ì „í™˜ìœ¨ ë†’ì€ í‚¤ì›Œë“œ').italic = True

        if "ë¶„ì„ ì‹¤íŒ¨" not in (data.get('high_intent_kw', '') or ''):
            if (not is_english_country_dl) and data.get('high_intent_kw_local_list') and data.get('high_intent_kw_en_list'):
                pairs = list(zip(data['high_intent_kw_local_list'], data['high_intent_kw_en_list']))
                for local_kw, en_kw in pairs:
                    doc.add_paragraph(str(local_kw), style='List Number')
                    p = doc.add_paragraph()
                    p.paragraph_format.left_indent = Inches(0.35)
                    run = p.add_run(str(en_kw))
                    run.font.size = Pt(9)
                    run.font.color.rgb = RGBColor(138, 143, 152)
            else:
                keywords_list = [k.strip() for k in (data.get('high_intent_kw', '') or '').split(',') if k.strip()]
                for kw in keywords_list:
                    clean_kw = re.sub(r'[^\w\s\uAC00-\uD7A3\u0600-\u06FF.,!?%&()\-:;\'\"]', '', kw).replace('**', '')
                    doc.add_paragraph(clean_kw, style='List Number')
        else:
            doc.add_paragraph('í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨')

        doc.add_paragraph()
        doc.add_heading('ë¡±í…Œì¼ í‚¤ì›Œë“œ (SEO ìµœì í™”)', 1)
        doc.add_paragraph('People Also Ask ë° ê´€ë ¨ ê²€ìƒ‰ì–´ ê¸°ë°˜').italic = True
        if data.get('longtail_kw_local'):
            if (not is_english_country_dl) and data.get('longtail_kw_en'):
                for local_kw, en_kw in zip(data['longtail_kw_local'], data['longtail_kw_en']):
                    doc.add_paragraph(str(local_kw), style='List Number')
                    p = doc.add_paragraph()
                    p.paragraph_format.left_indent = Inches(0.35)
                    run = p.add_run(str(en_kw))
                    run.font.size = Pt(9)
                    run.font.color.rgb = RGBColor(138, 143, 152)
            else:
                for kw in data['longtail_kw_local']:
                    clean_kw = re.sub(r'[^\w\s\uAC00-\uD7A3\u0600-\u06FF.,!?%&()\-:;\'\"]', '', str(kw)).replace('**', '')
                    doc.add_paragraph(clean_kw, style='List Number')
        else:
            doc.add_paragraph('ë°ì´í„° ì—†ìŒ')

        doc.add_paragraph()
        doc.add_heading('Google Trends ì¸ê¸° ê²€ìƒ‰ì–´', 1)
        doc.add_paragraph('ì‹¤ì œ ì†Œë¹„ì ê²€ìƒ‰ í–‰ë™ ë°ì´í„°').italic = True
        if data.get('trends_kw'):
            if (not is_english_country_dl) and data.get('trends_kw_en'):
                for local_kw, en_kw in zip(data['trends_kw'], data['trends_kw_en']):
                    doc.add_paragraph(str(local_kw), style='List Number')
                    p = doc.add_paragraph()
                    p.paragraph_format.left_indent = Inches(0.35)
                    run = p.add_run(str(en_kw))
                    run.font.size = Pt(9)
                    run.font.color.rgb = RGBColor(138, 143, 152)
            else:
                for kw in data['trends_kw']:
                    clean_kw = re.sub(r'[^\w\s\uAC00-\uD7A3\u0600-\u06FF.,!?%&()\-:;\'\"]', '', str(kw)).replace('**', '')
                    doc.add_paragraph(clean_kw, style='List Number')
        else:
            doc.add_paragraph('ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨')

        doc.add_page_break()
        doc.add_heading('ë§ˆì¼€íŒ… ì½˜í…ì¸  (ìµœì¢…)', 1)

        # ì•„ë§ˆì¡´
        doc.add_heading('ì•„ë§ˆì¡´ ë¸”ë™ë³´ë“œ ë¶ˆë ›í¬ì¸íŠ¸', 2)
        clean_and_add_text(doc, amazon_txt)
        if not is_english_country_dl:
            doc.add_paragraph()
            doc.add_heading('English Meaning', 3)
            clean_and_add_text(doc, data.get('amazon_meaning_en') or data.get('amazon_en') or '')
            st.space("small")

        # ìì‚¬ëª°
        doc.add_paragraph()
        doc.add_heading('ìì‚¬ëª° ì œí’ˆ Description', 2)
        clean_and_add_text(doc, d2c_txt)
        if not is_english_country_dl:
            doc.add_paragraph()
            doc.add_heading('English Meaning', 3)
            clean_and_add_text(doc, data.get('d2c_meaning_en') or data.get('d2c_en') or '')
            st.space("small")

        # SNS
        doc.add_paragraph()
        doc.add_heading('SNS ë§ˆì¼€íŒ… í”¼ë“œ', 2)
        clean_and_add_text(doc, sns_txt)
        if not is_english_country_dl:
            doc.add_paragraph()
            doc.add_heading('English Meaning', 3)
            clean_and_add_text(doc, data.get('social_meaning_en') or data.get('social_en') or '')
            st.space("small")

        output = BytesIO()
        doc.save(output)
        output.seek(0)
        return output


    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    col_dl1, col_dl2 = st.columns(2)
    
    excel_file = create_excel_report_dynamic(edited_amazon, edited_d2c, edited_sns)
    col_dl1.download_button(
        label="ğŸ“Š ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        data=excel_file,
        file_name=f"Marketing_Report_{data['english_kw']}_{saved_country}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True
    )
    
    word_file = create_word_report_dynamic(edited_amazon, edited_d2c, edited_sns)
    col_dl2.download_button(
        label="ğŸ“„ ì›Œë“œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        data=word_file,
        file_name=f"Marketing_Report_{data['english_kw']}_{saved_country}.docx",
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        use_container_width=True
    )

# --- Footer ---
st.divider()
st.markdown("""
<div style='text-align: center; color: #718096; font-size: 0.9em;'>
    <p>Global E-commerce All In One Solution</p>
    <p>Developed by Seyeon Global Connect</p>
</div>
""", unsafe_allow_html=True)